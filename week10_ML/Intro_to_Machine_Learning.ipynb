{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69c9b0c",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "This is a long tutorial that will explore the basic applications of machine learning algorithms. The only part that is required to be turned in is the exercise section at the end. You are, of course, encouraged to look into the subject farther on your own but for this course, only a shallow understanding is expected/required to complete the activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b58f8",
   "metadata": {},
   "source": [
    "# (1) Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e92a4",
   "metadata": {},
   "source": [
    "## First Things First\n",
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # support for arrays and vectorized operations\n",
    "import pandas as pd # support for DataFrames and Series\n",
    "import matplotlib.pyplot as plt # main graphing library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e41079",
   "metadata": {},
   "source": [
    "## Getting Sample Data\n",
    "sklearn, the machine learning package we will be using has a number of toy datasets within it to show different models or compare algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a466db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "wine = datasets.load_wine()\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881314a2",
   "metadata": {},
   "source": [
    "# (2) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403b8aa",
   "metadata": {},
   "source": [
    "It's important that we preprocess our data before we make machine learning techniques. We've already seen one type of preprocessing technique before, masking out NA values. Getting rid of NA values is essential as many ML algorithms can't handle them and will throw an error. \n",
    "\n",
    "There are two other types of preprocessing techniques which we will now go over:\n",
    "1. Encoding - This means turning a categorical variable into a numeric veriable. ML algorithms can only work with numbers, and to allow it to understand qualitative variables, we convert them into numbers. The number itself doesn't matter. E.g. [Male, Female] could become [0, 1] or [1, 0] and the ML algorithm would treat them the same. Generally, the easiest way to do this is to just assign each of the possible values of the variable to a number sequentially, a method called Ordinal Encoding.\n",
    "<br></br>\n",
    "2. Scaling - This means reshaping the data to account for difference in distribution between variables. In this case, we are using standard scaling so that we remove the mean and scale the data to unit variance. What we end up with is a range of values preserve the relations between the original data points but is centered at 0.\n",
    "\n",
    "Encoding may be a little easier to understand why it is necessary but to demonstrate why scaling is necessary, here is a short demo using dimensionality reduction techniques which we will explore late (separation of colors is the goal here)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69006495",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75a13c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "fix, axs = plt.subplots(1, 2, constrained_layout=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "embedding_unscaled = UMAP().fit_transform(wine.data)\n",
    "\n",
    "scaled_data = scaler.fit_transform(wine.data)\n",
    "embedding_scaled = UMAP().fit_transform(scaled_data)\n",
    "\n",
    "axs[0].scatter(embedding_unscaled[:, 0], embedding_unscaled[:, 1], c=wine.target, alpha=0.5, cmap='nipy_spectral')\n",
    "axs[0].set_title('No Scaling')\n",
    "\n",
    "axs[1].scatter(embedding_scaled[:, 0], embedding_scaled[:, 1], c=wine.target, alpha=0.5, cmap='nipy_spectral')\n",
    "axs[1].set_title('Scaled Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418229ec",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "random_categorical = pd.DataFrame(\n",
    "    {\n",
    "    'gender' : ['male', 'male', 'female', 'male', 'female'],\n",
    "    'smoking_status' : ['yes', 'no', 'yes', 'yes', 'no'],\n",
    "    'age' : [36, 24, 57, 24, 13]\n",
    "    }\n",
    ")\n",
    "\n",
    "random_categorical # sample dataset with categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0169cea",
   "metadata": {},
   "source": [
    "To encode (change from categorical to numerical) we can use OrdinalEncoder(). This works well when we have either a categorical variable with directional values or if we only have two possible values. Ensure that you are ONLY passing it the categorical columns (in this case, we DON'T want to pass it the age column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "unencoded_columns = random_categorical.loc[:, ['gender', 'smoking_status']]\n",
    "encoded_columns = encoder.fit_transform(unencoded_columns)\n",
    "random_categorical.loc[:, ['gender', 'smoking_status']] = encoded_columns\n",
    "\n",
    "random_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10178f84",
   "metadata": {},
   "source": [
    "To switch back from numerical to categorical, we use the inverse function. Your encoder must be the original encoder model you used to first encode it, or it will not be able to revert it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaaf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = encoder.inverse_transform(random_categorical.loc[:, ['gender', 'smoking_status']])\n",
    "random_categorical.loc[:, ['gender', 'smoking_status']] = original_columns\n",
    "\n",
    "random_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b30c5",
   "metadata": {},
   "source": [
    "# (3) Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf28ff4",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "With supervised learning, we start with labeled data, split a portion of it off (test set) and use the other portion (train set) to train our model. We can then validate our model by using it on the test set and seeing how well it does.\n",
    "<br></br>\n",
    "sklearn makes this easy for us by providing a function which will do exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a23fd",
   "metadata": {},
   "source": [
    "train_test_split takes three parameters\n",
    "+ data - the data which will be split\n",
    "+ target - the identified labels (must be numeric)\n",
    "+ train_size - the percent which will be used to train versus test, commonly between 50/50 to 80/20, but the choice is ultimately up to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba57504",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target # 0, 1, 2 correspond to Iris satosa, Iris versicolour, Iris virginica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c724d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data, columns=iris.feature_names) # does not have to be in DF or have column names\n",
    "# just in DF for ease of viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fb266",
   "metadata": {},
   "source": [
    "Note that X (capital) refers to the input dataframe and y (lowercase) refers to the labels, this notation is specific to supervised learning and should not be confused with the data x and y which we use for plotting.\n",
    "\n",
    "train_test_split gives 4 outputs:\n",
    "+ X_train - training data set\n",
    "+ X_test - testing data set\n",
    "+ y_train - training labels\n",
    "+ y_test - testing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed984d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.75)\n",
    "print(X_train.shape) # dataset split by train size\n",
    "print(X_test.shape) # dataset not included in above dataset\n",
    "print(y_train.shape) # labels for corresponding X_train\n",
    "print(y_test.shape) # labels for corresponding X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3bf8b",
   "metadata": {},
   "source": [
    "## Classification\n",
    "There are many different classifier models. Here, we will explore Nearest Neighbors, one of the most widely used methods, as well as Decision Trees, Multilayer Perceptron, and Naive Bayes.\n",
    "<br></br>\n",
    "For more info, the sklearn website has a great implementation of visualizing different classifiers. https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
    "<br></br>\n",
    "For a brief overview of each classifier:\n",
    "+ **Nearest Neighbors** - Imagine we had a 2D dataset with an x axis and a y axis. For our training data, we would classify each point based on the its label. This is our model. For each point in the test set, we get the labels of the nearest points (from the training set) on our XY plane and classify it based on the labels of its neighbors. While a 2D plane is easy to visualize, Nearest Neighbors can be used in any number of dimensions/features.\n",
    "<br></br>\n",
    "+ **Decision Tree** - This classifier builds what is called a decision tree based on the data. Imagine a fork in the road with two possible routes to go, and then within each route is an additional fork each with their own additional forks and so on. Now instead of forks in the road, imagine a tree with different limbs that branch out the further you go from the trunk. The base of the tree is where the input goes in and based on the model, it winds up at the very end of a possible branch which is the output label. This is a good way to visualize how decision tree algorithms work.\n",
    "<br></br>\n",
    "+ **Multilayer Perceptron** - This is a neural network based classifier. A neural network with a predetermined number of intermediate layers is created based on the training data.\n",
    "<br></br>\n",
    "+ **Naive-Bayes** - This is algorithm based on the independence clause in Bayes' Theorem. Each feature (column/variable) is assumed to be independent of every other feature. A probabilistic model is then built around this to arrive at the most likely output for every given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # default number of neighbors looked at is 5\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c146a",
   "metadata": {},
   "source": [
    "Basic Skeleton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf97f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Process Data First!!!\n",
    "classifier = FILL IN # substitute with any classifier\n",
    "classifier.fit(X_train, y_train) # trains model to training dataset, labels\n",
    "y_pred = classifier.predict(X_test) # uses modelto predict labels of X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c3bb6",
   "metadata": {},
   "source": [
    "Doing this yields the array y_pred which is an array of numeric labels that the model has decided. You can compare this with y_test to see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred == y_test) / len(y_test) # percent correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3c867",
   "metadata": {},
   "source": [
    "Example using KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.8)\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "\n",
    "print(f'Using the KNeighborsClassifier, the model was able to accurately predict {accuracy * 100}% of the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02adcc",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbf146",
   "metadata": {},
   "source": [
    "Regression is very much like classification however instead of starting with numeric data and ending with a categorical label, we start with numeric data and end with a numeric output.\n",
    "<br></br>\n",
    "We will be exploring Linear Regression, perhaps the simplest model, as well as Nearest Neighbors Regression, Stochastic Gradient Descent Regression, Support Vector Machine, and Bayesian Ridge Regression.\n",
    "<br></br>\n",
    "Due to the complex math behind each of these models, we will not go into details but the sklearn website is a great resource for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a631",
   "metadata": {},
   "source": [
    "The code behind regression is the exact same as in classification (SEE ABOVE) except instead of y_pred being a collection of numerical labels, it will be a numerical quantitative output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc820b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(diabetes.data)\n",
    "\n",
    "regressors_names = [\n",
    "    'Linear', \n",
    "    'Nearest Neighbors', \n",
    "    'Stochastic Gradient Descent', \n",
    "    'Support Vector',\n",
    "    'Bayesian Ridge'\n",
    "]\n",
    "\n",
    "regressors = [\n",
    "    LinearRegression(),\n",
    "    KNeighborsRegressor(),\n",
    "    SGDRegressor(),\n",
    "    SVR(),\n",
    "    BayesianRidge()\n",
    "]\n",
    "\n",
    "regressors_perf = {\n",
    "    0: [],\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: []\n",
    "}\n",
    "\n",
    "for n in range(1000):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_data, diabetes.target, train_size=0.8)\n",
    "    for i in range(len(regressors)):\n",
    "        model = regressors[i]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        difference = y_pred - y_test\n",
    "        regressors_perf[i].append(abs(np.mean(difference)))\n",
    "    if n % 50 == 0:\n",
    "        print(f'Completed {n}/1000 simulations')\n",
    "        \n",
    "print('\\nAfter 1000 simulations, the average error for each regressor is as follows:')\n",
    "for i in regressors_perf:\n",
    "    print(f'\\t{regressors_names[i]} : {np.mean(regressors_perf[i])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0d07c",
   "metadata": {},
   "source": [
    "# (4) Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27313d7f",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Dimensionality reduction is a technique used to take a dataset with a lot of different dimensions (e.g. columns/features) and reduce each data point down to a lower dimensional space. In this case, we will be reducing our data down to 2D so we can plot it on the XY plane. It's important to note that when dimensions are reduced, the X and Y axis don't represent any specific feature, but rather represent alikeness across every dimension/feature.\n",
    "<br></br>\n",
    "The three methods we will be using are in order of their complexity and development data. All three are established in research.\n",
    "+ Principal Component Analysis (PCA)\n",
    "+ t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "+ Uniform Manifold Approximation and Projection (UMAP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee968c93",
   "metadata": {},
   "source": [
    "### Skeleton Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b019cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Process Data First!!!\n",
    "reducer = FILL IN\n",
    "embedding = reducer.fit_transform(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060937d",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = embedding[:, 0]\n",
    "y_vals = embedding[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869eb9e2",
   "metadata": {},
   "source": [
    "Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deef61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(wine.data)\n",
    "\n",
    "reducers = [\n",
    "    PCA(),\n",
    "    TSNE(),\n",
    "    UMAP()\n",
    "]\n",
    "reducers_names = ['PCA', 't-SNE', 'UMAP']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, constrained_layout=True)\n",
    "\n",
    "for i in range(3):\n",
    "    embedding = reducers[i].fit_transform(scaled_data)\n",
    "    x_vals = embedding[:, 0]\n",
    "    y_vals = embedding[:, 1]\n",
    "    axs[i].scatter(x_vals, y_vals, c=wine.target, cmap='nipy_spectral', alpha=0.5)\n",
    "    axs[i].set_title(reducers_names[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a3fbd",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "There are a wide variety of different clustering algorithms. Here, we will only examine K-Means, perhaps the most commonly used clustering algorithm, and HDBSCAN, an improved version of the standard DBSCAN algorithm, which I think is one of the best if the number of clusters is unknown :)\n",
    "<br></br>\n",
    "But here is a link to the sklearn implementation of different clustering algorithms with some helpful visualizations of the differences between each algorithm.\n",
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097c0b1",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "By default, K-Means will attempt to make a guess as to how many clusters there are based on similarity in location. The number of clusters then, will vary. You can however manually set the number of clusters using the n_clusters parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, constrained_layout=True)\n",
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(wine.data)\n",
    "\n",
    "reducer = PCA() # arbitrary for this demo\n",
    "embedding = reducer.fit_transform(scaled_data)\n",
    "x_vals = embedding[:, 0]\n",
    "y_vals = embedding[:, 1]\n",
    "\n",
    "param_names = ['No Params', 'Clusters = 3', 'Clusters = 15']\n",
    "cluster_models = [\n",
    "    KMeans(),\n",
    "    KMeans(n_clusters=3),\n",
    "    KMeans(n_clusters=15)\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "    labels = cluster_models[i].fit_predict(embedding)\n",
    "    \n",
    "    axs[i].scatter(x_vals, y_vals, c=labels, cmap='nipy_spectral', alpha=0.5)\n",
    "    axs[i].set_title(param_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e09f2",
   "metadata": {},
   "source": [
    "### Elbow Plots\n",
    "How do we figure out whow many clusters to use for KMeans? One way to do this is to create an elbow plot. An elbow plot shows a metric called inertia for various different possible K values. A higher K means a lower inertia score the lower the inertia score the better, but there is a point of diminishing returns. Our aim is to find the number of clusters, K, that results in the best clusters/inertia ratio. You can find this by visualizing the plot as an arm and trying to find the spot that is the elbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41169b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow plot method 1\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(wine.data)\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(scaled_data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8780484",
   "metadata": {},
   "source": [
    "yellowbrick, another package, provides a very convenient visualizer function which automatically graphs an elbow plot and tells you what the best number of clusters is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow  plot method 2\n",
    "visualizer = KElbowVisualizer(KMeans(), k=(1,11))\n",
    "visualizer.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02b517",
   "metadata": {},
   "source": [
    "### Hierarchical Density-Based Spatial Clustering of Applications with Noise\n",
    "\"HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander. It extends DBSCAN by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters.\"\n",
    "https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\n",
    "<br></br>\n",
    "Unlike K-Means clustering, HDBSCAN does not allow you to choose the default number of clusters, but if you do not know the exact amount of clusters, it generally performs better than other algorithms. In addition, unlike K-Means clustering, HDBSCAN does not need to assign every value to a label, and you can see the label -1 used when it is not confident about which cluster the data belongs to (this is shown as dark grey on the plot).\n",
    "<br></br>\n",
    "Due to the method in which the algorithm works, HDBSCAN is not a good choice for clustering based on PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 5]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(wine.data)\n",
    "\n",
    "cluster_model = HDBSCAN()\n",
    "\n",
    "reducers = [\n",
    "    PCA(),\n",
    "    TSNE(),\n",
    "    UMAP()\n",
    "]\n",
    "reducers_names = ['PCA', 't-SNE', 'UMAP']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, constrained_layout=True)\n",
    "\n",
    "for i in range(3):\n",
    "    embedding = reducers[i].fit_transform(scaled_data)\n",
    "    x_vals = embedding[:, 0]\n",
    "    y_vals = embedding[:, 1]\n",
    "    labels = cluster_model.fit_predict(embedding)\n",
    "    axs[i].scatter(x_vals, y_vals, c=labels, cmap='nipy_spectral', alpha=0.5)\n",
    "    axs[i].set_title(reducers_names[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbe410",
   "metadata": {},
   "source": [
    "# (5) Sidenote on Hyperparameters\n",
    "Nearly every machine learning has a variety of different variables to finetune the performance. Certain algorithms are better than others using the default parameters, but to find the absolute best one, you have to play around. These variables are called hyperparameters and there is good documentation on the scikit-learn/UMAP/HDBSCAN websites. \n",
    "<br></br>\n",
    "There are also a number of functions within sklearn to find the best parameter. This will not be covered in this course and you are not expected to use them, but if you are interested, here is a link to their documentation on finding the best model: https://scikit-learn.org/stable/model_selection.html#model-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab055406",
   "metadata": {},
   "source": [
    "# (6) Implementation With -Omics Data\n",
    "Omics data works the exact same as the other data in this tutorial. Just remember to identify which machine learning technique to use, and to use every preprocessing step we talked about:\n",
    "+ Clean NA values\n",
    "+ Encode categorical variables into numeric labels\n",
    "+ Scale data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2303f1f",
   "metadata": {},
   "source": [
    "# (7) Exercises - only this part will be graded\n",
    "Exercises 1 & 2 will be graded on accuracy. Exercise 3 will be graded on completion and effort. No extra credit will be granted outside of Exercise 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0da24",
   "metadata": {},
   "source": [
    "**Exercise 1** - Preprocessing/Supervised Learning (3 pt)\n",
    "1. Fill in the skeleton code to complete the iris classification plot.\n",
    "2. Answer the following questions:\n",
    "+ Define the following preprocessing methods and describe why each are necessary\n",
    "    + NA removal\n",
    "    + Scaling\n",
    "    + Encoding\n",
    "+ What is the difference between classification and regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc880aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "unscaled_data = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = FILL IN HERE\n",
    "\n",
    "classifier = FILL IN HERE # your choice of which classifier to use\n",
    "\n",
    "FILL IN HERE = train_test_split(scaled_data, iris.target, train_size=0.75)\n",
    "\n",
    "classifier.fit(FILL IN HERE)\n",
    "\n",
    "y_pred = FILL IN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f5f45",
   "metadata": {},
   "source": [
    "Visualize your predictions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "iris_df = pd.DataFrame(X_test)\n",
    "\n",
    "axs[0][0].scatter(iris_df.iloc[:, 0], iris_df.iloc[:, 1], c=y_pred, s=20, cmap='gist_rainbow')\n",
    "axs[0][0].set_ylabel('sepal width (cm)')\n",
    "\n",
    "axs[0][1].scatter(iris_df.iloc[:, 2], iris_df.iloc[:, 1], c=y_pred, s=20, cmap='gist_rainbow')\n",
    "\n",
    "axs[1][0].scatter(iris_df.iloc[:, 0], iris_df.iloc[:, 3], c=y_pred, s=20, cmap='gist_rainbow')\n",
    "axs[1][0].set_xlabel('sepal length (cm)')\n",
    "axs[1][0].set_ylabel('petal width (cm)')\n",
    "\n",
    "axs[1][1].scatter(iris_df.iloc[:, 2], iris_df.iloc[:, 3], c=y_pred, s=20, cmap='gist_rainbow')\n",
    "axs[1][1].set_xlabel('petal length (cm)')\n",
    "\n",
    "fig.suptitle('Comparing Length and Width Between Sepals and Petals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadea028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313dedef",
   "metadata": {},
   "source": [
    "**Exercise 2** - Unsupervised Learning (2 pt)\n",
    "1. Fill in the skeleton code to complete the iris dimensionality reduction and clustering plot.\n",
    "2. Answer the following questions:\n",
    "+ What do the axes on a UMAP, t-SNE, or PCA plot represent?\n",
    "+ Compare the clustered plot vs the actual labels, how close were the clustered labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1) Dimensionality Reduction\n",
    "from umap import UMAP\n",
    "\n",
    "reducer = UMAP()\n",
    "\n",
    "embedding = FILL IN HERE # use the scaled_data from exercise 1 for the data\n",
    "\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2) Determination of # of Clusters for KMeans()\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(FILL IN HERE)\n",
    "visualizer.fit(FILL IN HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3) KMeans Clustering\n",
    "cluster_model = KMeans(n_clusters = FILL IN HERE)\n",
    "\n",
    "labels = cluster_model.fit_predict(embedding)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, constrained_layout=True, sharey=True)\n",
    "\n",
    "axs[0].scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='nipy_spectral', alpha=0.5)\n",
    "axs[0].set_title('Clustered')\n",
    "axs[1].scatter(embedding[:, 0], embedding[:, 1], c=iris.target, cmap='gnuplot', alpha=0.5)\n",
    "axs[1].set_title('Actual Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd5be7",
   "metadata": {},
   "source": [
    "**Exercise 3** - Do It Yourself (5 pt)\n",
    "<br></br>dermatology.csv is a CSV file with 34 different features. The exact description of each feature is in the dermatology.txt file. Each patient has one of the following conditions (this is the 35th column):\n",
    "+ psoriasis (1)\n",
    "+ seboreic dermatitis (2)\n",
    "+ lichen planus (3)\n",
    "+ pityriasis rosea (4)\n",
    "+ chronic dermatitis (5)\n",
    "+ pityriasis rubra pilaris (6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b671f",
   "metadata": {},
   "source": [
    "Part I - Preprocessing (1 pt)\n",
    "1. Load the data in\n",
    "2. Separate the target column\n",
    "3. Preprocess the data using necessary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78837efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3ca60",
   "metadata": {},
   "source": [
    "Part II - Supervised Learning (2 pt)\n",
    "1. Create a train-test split with an appropriate split-ratio\n",
    "2. Test at least 3 different classification techniques to determine which is best able to predict skin condition and present it in a graphical method (can be separate or joined figures)\n",
    "3. Use the age column as a target and create a regression model to predict age from the other features (include skin condition as a feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e97509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a4366",
   "metadata": {},
   "source": [
    "Part III - Unsupervised Learning (2 pt)\n",
    "1. Use all three dimensionality-reduction techniques to show the data from Part I in 2D (can be separate or joined figures) Pick the dimensionality-reduction technique which results in the greatest separation\n",
    "2. Create an elbow plot to determine the K number of clusters for K-Means\n",
    "3. Cluster based on K-Means and HDBSCAN in a figure which shows both methods side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc27a09",
   "metadata": {},
   "source": [
    "Part IV - Brief Response (REQUIRED IN ORDER TO GET CREDIT FOR EACH OTHER PART)\n",
    "<br></br>\n",
    "Answer the following questions if you completed the part:\n",
    "1. Which of the preprocessing techniques were required for the data? How did you know?\n",
    "2. Which supervised learning technique (classification vs regression) was best able to predict its respective target? Why do you think so?\n",
    "3. Interpret what the distance between each point represents on the reduced dimensions figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca15104",
   "metadata": {},
   "source": [
    "**Exercise 4** - Challenge - Extra Credit (5 pts possible) <br>\n",
    "</br>\n",
    "INDIVIDUAL WORK ONLY - credit will be awarded based on thoughtful completion and demonstration of an understanding of ML principles, <u>code comments must be included</u>\n",
    "<br></br>mushroom.csv is a dataset includes 61069 hypothetical mushrooms with caps based on 173 species (353 mushrooms per species). Each mushroom is identified as edible or poisonous. Of the 20 variables, 17 are nominal and 3 are metrical. The features and dataset design can be found in mushroom.txt. Your task is as follows:\n",
    "+ Load and process the data (one challenge we often find ourselves facing is having to reformat data, this will be an exercise in that) from the .csv file\n",
    "+ Create a classifier design and test that it can correctly identify whether a mushroom is edible or poisonous with a minimum accuracy of no lower than 95%. You will determine this by testing it on a 60/40 train-test split 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
